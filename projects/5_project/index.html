<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Advancing thoracic disease classification with CNNs | Minye Zhou </title> <meta name="author" content="Minye Zhou"> <meta name="description" content="BMI707 group project"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://minyezhou429.github.io/projects/5_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Minye</span> Zhou </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/_pages/grades/">grades </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Advancing thoracic disease classification with CNNs</h1> <p class="post-description">BMI707 group project</p> </header> <article> <p><strong>Title</strong>:</p> <p>Advancing Thoracic Disease Identification with CNNs</p> <p><strong>Members and Contributors</strong>:</p> <p>Minye Zhou, Michelle Dai, Evelyn Jiang</p> <p><strong>Data type</strong>:</p> <p>Medical Images</p> <p><strong>Summary of Dataset</strong>:</p> <p>We used the ChestX-Ray14 dataset. This dataset contains 112,120 frontal-view X-ray images from 30,805 distinct patients, each potentially labeled with one or more of fourteen identified diseases. These labels were extracted from corresponding radiological reports using natural language processing techniques, which are expected to have an accuracy greater than 90%. The dataset identifies fourteen prevalent thoracic conditions using number 0 to 14: No Finding: 0, Atelectasis: 1, Cardiomegaly: 2, Effusion: 3, Infiltration: 4, Mass: 5, Nodule: 6, Pneumonia: 7, Pneumothorax: 8, Consolidation: 9, Edema: 10, Emphysema: 11, Fibrosis: 12, Pleural_Thickening: 13, Hernia: 14.</p> <p>We split the data into train and test sets, containing 8000 and 2000 examples respectively. We found that among 8000 training samples, 1187 samples have multiple labels. For 2000 testing samples, 304 samples have multiple labels (more than one thoracic condition). We excluded those multi-labeled samples. Additionally, we allocated 25% of the data from the training set to serve as a validation set to monitor and tune the model’s performance during training.</p> <p><strong>Introduction</strong>:</p> <p>Chest X-ray imaging stands as a cornerstone in radiology. With advanced deep learning technologies, significant strides have been made in the automated analysis of medical images. Particularly, advancements in disease classification through these methods have seen considerable improvements over traditional approaches. Our study harnesses the “ChestX-ray14” dataset, with its detailed labeling, to investigate disease classification and pathology localization within X-ray images.</p> <p>The primary objective of our research is to evaluate the precision of various Convolutional Neural Network (CNN) architectures in identifying specific thoracic diseases from X-ray images. This involves a comprehensive comparison of the performance of different CNN models to assess their efficacy in classifying a broad spectrum of thoracic conditions. Furthermore, our study seeks to adapt and finetune pre-trained ImageNet models for this task.</p> <p><strong>Methods</strong>:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cnn_pipeline-480.webp 480w,/assets/img/cnn_pipeline-800.webp 800w,/assets/img/cnn_pipeline-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/cnn_pipeline.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Data preprocessing:</p> <p>The datasets have been downloaded and formatted into appropriately pre-processed floating point tensors utilizing <code class="language-plaintext highlighter-rouge">Keras</code> packages. In particular, we employed the <code class="language-plaintext highlighter-rouge">class ImageDataGenerator</code> to retrieve and decode from PNG images (1024x1024) to RGB pixel grids, further transforming them into floating point tensors. Then, the pixel values were normalized from their original range of 0 to 255 to a new scale of [0, 1], which is optimal for neural network processing.</p> <p>The ImageDataGenerator yields batches of 224x224 RGB images and 15 categorical labels. The batch size is 80. The resulting data batch shape is (80,224,224,3). Labels batch shape is (80,15)</p> <p>Base Convolutional Network:</p> <p>We first build a base convolutional network with four convolutional layers with each layer followed by a max pooling layer, and two dense layers. The activation function for intermediate layers is ReLU. We used softmax as an activation function for the top layer because softmax could give our model partial credits, thus increasing training effectiveness. The model is summarized in figure below(left).</p> <p>Since we have 15 classes, we use categorical cross entropy as a loss function where the target class is a label in a one-hot encoded vector. RMSprop with a learning rate of 2e-5 is used to optimize the loss. We used an accuracy metric to assess model performance. The model is trained for 30 epochs. For each epoch, 63 steps are used to yield from the training generator before declaring one epoch finished and starting the next epoch. 22 steps are used to yield from the validation generator.</p> <p>Pre-trained Convolutional Network:</p> <p>Then, we applied transfer learning on a pre-trained VGG19 model from ImageNet, with which we froze all layers except for the top classification layers to adapt the network to the specific features of our dataset. The conv_base VGG19 model is summarized in Appendix Figure 2.</p> <p>After the VGG19 base, the model structure includes one flattened layer, a fully connected layer with 512 units and ReLU activation, a dropout layer with a rate of 0.5 to help prevent overfitting, as well as the final output layer with 15 units corresponding to the number of disease classes, using softmax activation to output probabilities of each class. Similar to our custom CNN, we utilized the same optimizer and loss function for this multi-class classification task. The model is trained for 10 epochs.</p> <p>In our second transfer learning model, we attempted a similar structure as above on a pre-trained Xception model on ImageNet. After the Xception base, the model structure includes one flattened layer, two fully connected layers with 512 units, and ReLU activation, a dropout layer with a rate of 0.5. In the first dense layer after the base, an elastic net kernel regularizer has been added to prevent further overfitting (l1 = 1e-4, l2 = 1e-4). The output layer, optimizer, loss function, and number of training epochs are the same as our first transfer learning model.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cnn_models-480.webp 480w,/assets/img/cnn_models-800.webp 800w,/assets/img/cnn_models-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/cnn_models.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The evaluation metrics of the deep learning models:</p> <p>AUC, Accuracy, F1 score, precision, and recall are used to evaluate the performance of our models. Accuracy is the simplest and most intuitive performance measure. It is defined as the ratio of correctly predicted observations to the total observations. Accuracy itself is not sufficient since it may be misleading for an imbalanced dataset. Precision measures the accuracy of positive predictions. Formulated as the ratio of true positives to the sum of true and false positives. Recall measures the ability of a model to find all the true positives within a dataset. Defined as the ratio of true positives to the sum of true positives and false negatives. Together, these metrics help in evaluating the model’s performance when dealing with imbalanced datasets, offering insights into the trade-offs between identifying all relevant instances and maintaining a high level of prediction precision. The F1 Score is the harmonic mean of precision and recall, taking both false positives and false negatives into account. AUC measures separability. It tells how much the model is capable of distinguishing between classes. The higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s. By measuring the area under the curve, the AUC metric captures the overall performance of the model across all classification thresholds, thus providing a robust indication of its effectiveness.</p> <p><strong>Results</strong>:</p> <p>Deep learning models:</p> <p>On the training set, the Base CNN has an accuracy of 0.6855 and a comparatively low loss of 1.2976. It achieved a high AUC of 0.9144. The precision and recall were closely matched at 0.6864 and 0.6786, respectively, leading to a balanced F1 score of 0.6825. VGG19 has a slightly lower accuracy of 0.6749 and a higher loss of 4.5783. Although its AUC dropped to 0.8546, it maintained a reasonable precision of 0.6845 and a recall of 0.6721, resulting in an F1 score of 0.6782. The Xception model, on the other hand, matched the Base CNN in accuracy at 0.6855 but suffered from a higher loss of 4.1506. Its AUC of 0.9013 was commendable, yet a notable disparity between a lower precision of 0.47 and a higher recall of 0.69 led to a reduced F1 score of 0.56, highlighting issues with generating too many false positives.</p> <p>On the testing set, the Base CNN maintained consistent performance with a slight decrease in accuracy to 0.6810, a loss of 1.3701, and an AUC similar to training at 0.9142. The precision and recall remained well-balanced at 0.6805 and 0.6757, respectively, with an F1 score of 0.6781, showing that the model performs similarly well under testing conditions. VGG19 also achieved an accuracy of 0.6810 but with a higher loss of 1.7703. Its AUC decreased to 0.8933, yet it managed to slightly improve both precision and recall to 0.6810, leading to a higher F1 score of 0.6809 compared to training, indicating slight enhancements or adaptability in test scenarios. Xception maintained an accuracy of 0.6810 but with an increased loss of 4.3375 and an AUC of 0.8850. The precision dropped further to 0.46 while recall was at 0.68, resulting in a lower F1 score of 0.55.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cnn_deep-480.webp 480w,/assets/img/cnn_deep-800.webp 800w,/assets/img/cnn_deep-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/cnn_deep.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cnn_performance-480.webp 480w,/assets/img/cnn_performance-800.webp 800w,/assets/img/cnn_performance-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/cnn_performance.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Non-deep learning models:</p> <p>On the test set, the KMeans clustering approach has a low accuracy of 0.125, complemented by a precision of 0.57 and a recall of 0.12, resulting in an F1 score of 0.17. These metrics indicate that while KMeans was good at identifying specific classes when it predicted correctly, its overall ability to accurately label the dataset was significantly lacking, as evidenced by the low recall and, consequently, the poor F1 score. In contrast, the KNN (n=3) algorithm performed better, achieving an accuracy of 0.6445. Despite a lower precision of 0.49, the recall was considerably higher at 0.64, which led to a more balanced F1 score of 0.55. On the testing set, the performance trends observed were similar. KMeans clustering again showed limited effectiveness, with an accuracy of only 0.1047, precision at 0.54, and recall at 0.10, culminating in an F1 score of 0.14. On the other hand, KNN (n=3) has better performance in the training set, with an accuracy of 0.7144. It maintained a precision of 0.52 and an improved recall of 0.71, leading to a higher F1 score of 0.64.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cnn_nondeep-480.webp 480w,/assets/img/cnn_nondeep-800.webp 800w,/assets/img/cnn_nondeep-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/cnn_nondeep.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Limitations of the study:</p> <p>In this study, we have eliminated multi-labeled samples that could result in the loss of valuable information useful for more detailed classifications or for understanding the underlying complexities of the data. This could limit the depth of insights our model can provide.</p> <p>While the Base CNN shows promising results, the generalization capability of these pre-trained models to new, unseen data remains uncertain. The models’ performance metrics are only as good as the data they are trained on, and if the training data lacks diversity or is not representative of the broader population, the model might not perform as well in real-world settings.</p> <p>Challenges:</p> <p>Even with strategies like stratified sampling and weight class, managing class imbalance can still pose significant challenges. While these methods help in creating a more balanced dataset for training models, they may only guarantee that the model will generalize well to real-world data, where the class distribution may still be highly skewed.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Minye Zhou. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of my projects inside/outside classes.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-grades",title:"grades",description:"",section:"Navigation",handler:()=>{window.location.href="/_pages/grades/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:"Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra",description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:"Displaying External Posts on Your al-folio Blog",description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-github-metadata",title:"a post with github metadata",description:"a quick run down on accessing github metadata.",section:"Posts",handler:()=>{window.location.href="/blog/2020/github-metadata/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"projects-gene-regulatory-network",title:"Gene regulatory network",description:"Master Thesis project",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-leukemia-diagnostics",title:"Leukemia Diagnostics",description:"Undergraduate senior design project",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-fluids-resuscitation-of-patients-with-congestive-heart-failure-and-or-end-stage-renal-disease",title:"Fluids Resuscitation of Patients with Congestive Heart Failure and/or End Stage Renal Disease",description:"MIT HST.953 Group project",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-single-cell-analysis-of-breast-cancer-dataset",title:"Single Cell Analysis of Breast cancer dataset",description:"BMI710 Individual project",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-advancing-thoracic-disease-classification-with-cnns",title:"Advancing thoracic disease classification with CNNs",description:"BMI707 group project",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-rna-sequencing-analysis",title:"RNA sequencing analysis",description:"BST281 Group project",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6D%69%6E%79%65%7A%68%6F%75@%68%73%70%68.%68%61%72%76%61%72%64.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>